{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess the new MEMO following the exact same pre-processing steps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#essentials\n",
    "import pandas as pd\n",
    "\n",
    "#for cleaning and text-preprocessing\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read in the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed039eea-76e5-4a07-a4ff-30ed7a305e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the csv file\n",
    "df = pd.read_csv('/Users/carljohanson/Desktop/Speciale - Code Project/Code/data/InfoDesk/daily_MEMO_data', delimiter='\\t') #adjust path to your file\n",
    "\n",
    "#rename columns\n",
    "df.columns = ['column1', 'column2', 'column3']\n",
    "\n",
    "#concatenate the three columns into a new column with all data\n",
    "df['description'] = df['column1'] + ' ' + df['column2'] + ' ' + df['column3']\n",
    "\n",
    "#remove the three columns after concatenation\n",
    "df = df[df.columns[~df.columns.isin(['column1', 'column2', 'column3'])]]\n",
    "\n",
    "#get shape of dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Corpus cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7435752a-b98f-4e87-a533-ebf8a97897cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a regular expression pattern to match the XML tags and attributes\n",
    "pattern = re.compile(r'<.*?>')\n",
    "\n",
    "#remove missing values\n",
    "df.dropna(subset=['description'], inplace=True)\n",
    "\n",
    "#apply the regular expression pattern to remove the tags and attributes from the text\n",
    "df['description'] = df['description'].apply(lambda x: re.sub(pattern, ' ', x))\n",
    "\n",
    "#remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "print(df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Corpus cleaning & preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94abf40a-f298-4265-8b25-933396e1e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the text\n",
    "\n",
    "#remove certain words from the description column. Add more at will if necessary\n",
    "df['description'] = df['description'].str.replace('PRNewswire', '').str.replace('NASDAQ', '').str.replace('draft', '').str.replace('EINPresswire', '').str.replace('ResearchAndMarkets', '').str.replace('Inc','').str.replace('please','').str.replace('Inc','')\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    #remove punctuation and special characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "\n",
    "    #remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    #remove single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "    #convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    #remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "#apply the preprocess_text function to the text column of the dataframe\n",
    "df['description'] = df['description'].apply(preprocess_text)\n",
    "\n",
    "#remove duplicates for king knut\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "#remove missing values for king knut\n",
    "df.dropna(subset=['description'], inplace=True)\n",
    "\n",
    "#print the first 20 rows\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#remove rows/documents with less than 5 words\n",
    "df = df[df['description'].apply(lambda x: len(x.split()) >= 5)]\n",
    "\n",
    "#print the first 20 rows\n",
    "print(df.head(20))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Translate to English for uniformity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3366adba-f308-406a-b1df-20450fbe9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "#make language detection deterministic\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "#define language detection function\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "#define fallback language\n",
    "fallback_language = 'en'\n",
    "\n",
    "#define placeholder string\n",
    "placeholder = 'XXXXX'\n",
    "\n",
    "#detect language of 'description' column and translate text to English if necessary\n",
    "df['language'] = df['description'].apply(detect_language)\n",
    "df['description_no_nn'] = df['description'].str.replace('novo nordisk', placeholder)\n",
    "df['translated_text'] = ''\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # Replace 'zh-cn' with 'chinese (simplified)'\n",
    "    lang = 'chinese (simplified)' if row['language'] == 'zh-cn' else row['language']\n",
    "\n",
    "    if lang != 'en' and lang != 'unknown':\n",
    "        try:\n",
    "            translation = GoogleTranslator(source=lang, target='en').translate(row['description_no_nn'])\n",
    "            translation = translation.replace(placeholder, 'novo nordisk')\n",
    "        except Exception as e:\n",
    "            print(f\"Translation failed for index {index} with source language {lang}. Error: {e}\")\n",
    "            continue\n",
    "        df.at[index, 'translated_text'] = translation\n",
    "    else:\n",
    "        df.at[index, 'translated_text'] = row['description_no_nn'].replace(placeholder, 'novo nordisk')\n",
    "\n",
    "#print the first 20 rows\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a18876-4685-4bd7-a61c-43ed28747a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates for king knut\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "#remove missing values for king knut\n",
    "# Replace 'unknown' values with NaN\n",
    "df = df.replace('unknown', np.nan)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "#print the first 20 rows\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8848492-cbab-42a3-88bf-2a028c488926",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentences = df['translated_text']\n",
    "\n",
    "all_words = \"\".join(str(i) for i in sentences)\n",
    "\n",
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(all_words)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Text preprocessing & tokenization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ac6363-8dd4-402d-8f0a-2c4a513f08ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "#tokenization of the clean and translated text\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#defining a function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize text\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and \"'\" not in token and token not in string.punctuation]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "#apply the function to the translated text\n",
    "df['tokens_text'] = df['translated_text'].apply(preprocess_text)\n",
    "\n",
    "#show the tokens\n",
    "df['tokens_text'].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Text preprocessing & tokenization with bigrams and trigrams"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065367f9-e489-4d97-8e2d-2de5fac3ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import phrases\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "#text with bigrams, trigrams\n",
    "# English\n",
    "connector_words = phrases.ENGLISH_CONNECTOR_WORDS\n",
    "\n",
    "# Detect bigrams\n",
    "bigram = Phrases(df['tokens_text'], min_count=2, threshold=10, connector_words=connector_words)\n",
    "bigram_phraser = Phraser(bigram)\n",
    "\n",
    "# Detect trigrams\n",
    "trigram = Phrases(bigram_phraser[df['tokens_text']], min_count=2, threshold=10, connector_words=connector_words)\n",
    "trigram_phraser = Phraser(trigram)\n",
    "\n",
    "#defining a function to preprocess the text with bigrams and trigrams\n",
    "def preprocess_text_bigram_trigram(tokens):\n",
    "\n",
    "    # Detect and add bigrams and trigrams\n",
    "    tokens = bigram_phraser[tokens]\n",
    "    tokens = trigram_phraser[tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "#apply the function to the translated text\n",
    "df['preprocessed_text'] = df['tokens_text'].apply(preprocess_text_bigram_trigram)\n",
    "\n",
    "#show the tokens\n",
    "df['preprocessed_text'].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#remove rows with empty lists\n",
    "df = df[df.astype(str)['preprocessed_text'] != '[]']\n",
    "\n",
    "#remove rows with less than 5 words\n",
    "df = df[df['preprocessed_text'].map(len) > 5]\n",
    "\n",
    "#remove tokens with less than 3 characters\n",
    "df['preprocessed_text'] = df['preprocessed_text'].apply(lambda x: [item for item in x if len(item) > 3])\n",
    "\n",
    "#show the shape of the dataset\n",
    "df.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the LDA Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim import models\n",
    "\n",
    "temp_file = '/Users/carljohanson/Desktop/Speciale - Code Project/Code/Models/lda_model'\n",
    "lda_model = models.ldamodel.LdaModel.load(temp_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a new corpus with the same dictionary and update the LDA model, and pass the new corpus through the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#create a list of list to mirror the structure of the preprocessed text data\n",
    "doc_list = df['preprocessed_text'].tolist()\n",
    "\n",
    "#load the dictionary\n",
    "dictionary = Dictionary.load('/Users/carljohanson/Desktop/Speciale - Code Project/Code/Models/lda_model.id2word')\n",
    "\n",
    "#convert the preprocessed text data into a bag-of-words representation using the loaded dictionary\n",
    "corpus = [dictionary.doc2bow(doc) for doc in doc_list]\n",
    "\n",
    "#use the lda model to transform into a bag-of-words representation with topic distribution\n",
    "lda_corpus = lda_model[corpus]\n",
    "\n",
    "#update the lda model with the new corpus\n",
    "lda_model.update(corpus, passes=10, iterations=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluate the LDA model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#evaluate using coherence score and perplexity\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#compute Coherence Score\n",
    "cm = CoherenceModel(model=lda_model, corpus=lda_corpus, texts=doc_list, dictionary=dictionary, coherence='c_v')\n",
    "coherence = cm.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence)\n",
    "\n",
    "#compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  #The lower, the better."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Predict the dominant topic for each document and prepare the data for ML classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#loop through each document in the corpus and get its topic distribution\n",
    "topic_distributions = []\n",
    "for i, doc in enumerate(corpus):\n",
    "    topic_dist = lda_model.get_document_topics(doc, minimum_probability=0.0)\n",
    "    topic_distributions.append(topic_dist)\n",
    "\n",
    "#extract the dominant topic for each document and add it as a new column in the dataframe\n",
    "df['dominant_topic'] = [max(topic_dist, key=lambda x:x[1])[0] for topic_dist in topic_distributions]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#export topic proportions and labels to ML classifier\n",
    "\n",
    "#compute the topic proportions for each document in the corpus\n",
    "topic_proportions = []\n",
    "for doc in corpus:\n",
    "    topic_vector = lda_model.get_document_topics(doc, minimum_probability=0.0)\n",
    "    proportions = [topic_prob[1] for topic_prob in topic_vector]\n",
    "    topic_proportions.append(proportions)\n",
    "\n",
    "#add topic proportions to the dataframe\n",
    "df['topic_proportions'] = topic_proportions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['topic_proportions'].head(50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "#create a numpy array of the topic proportions\n",
    "X = np.array(df['topic_proportions'].tolist())\n",
    "\n",
    "#load the random forest model\n",
    "rf = joblib.load('/Users/carljohanson/Desktop/Speciale - Code Project/Code/Models/rf_model')\n",
    "\n",
    "#predict the topic for each document\n",
    "predict_topic = rf.predict(X)\n",
    "\n",
    "#display the predicted topic\n",
    "predict_topic"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#extract feature vectors and labels from the DataFrame\n",
    "X = np.array(df['topic_proportions'].tolist())  #features (document-topic distributions)\n",
    "y = df['dominant_topic'].values  #labels (categories/classes)\n",
    "\n",
    "#split the dataset 80:20 for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#predict the test set\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "#evaluate the model\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#convert to a picture\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=\".0f\", linewidths=.5, square=True, cmap='Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(accuracy_score(y_test, y_pred))\n",
    "plt.title(all_sample_title, size=15);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create dataframe with the predicted topic and export to csv file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#add the predicted topic to the dataframe\n",
    "df['topic'] = predict_topic\n",
    "\n",
    "final_df = df[['description','topic']].head(50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#export the dataframe to a csv file\n",
    "final_df.to_csv('/Users/carljohanson/Desktop/Speciale - Code Project/Code/Results/prepared_memo.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_df"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
