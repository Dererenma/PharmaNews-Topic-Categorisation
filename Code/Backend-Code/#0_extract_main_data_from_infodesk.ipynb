{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# General InfoDesk API interface. Created by Sidsel Boldsen & Louise Wille from Novo Nordisk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dateutil.relativedelta import relativedelta"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_request(start_date, end_date, profile=None, channel=None, limit=1000):\n",
    "\n",
    "    # Auth details. Novo Nordisk Credentials. Has to be changed to your own.\n",
    "    username = ####\n",
    "    password = ####\n",
    "\n",
    "    api_key = ####\n",
    "    realm = ####\n",
    "\n",
    "    # Get Bearer token\n",
    "    data = {\"username\":f\"{username}\",\"password\":f\"{password}\"}\n",
    "    headers = {'Content-Type': \"application/json\", \"accept\":\"application/json\",\n",
    "               \"X-API-KEY\":f\"{api_key}\", 'realm':f\"{realm}\"}\n",
    "    \n",
    "    url = \"https://api.infodesk.com/auth/v1/login?realm=infoauth.infodesk.com\"\n",
    "\n",
    "    bearer_token = json.loads(requests.post(url, json=data, headers=headers, verify=False).text)[\"access_token\"]\n",
    "    print(bearer_token)\n",
    "    #bearer_token = \"eyJ0eXAiOiJKV1QiLCJ6aXAiOiJERUYiLCJhbGciOiJSUzI1NiJ9.dVDLTsQgFP0X1sOUFgq0K3UzmcS4MO6MaXjVwWlhAoyPGP9d6DiJYyIrcs_jnnM_QTxK0IPN7fbambd45fyrdz5oG_dr5WewAuKobXoIQu2te97qTKatro1pMUQNE5BIjaFsMYWE0VbSkXedNFloY8zkXUqH2FdVjH5t3ei1OTn3hOAyrLw4pl1TBSOmOVbB-3T-F3oBL3TZOPm9cXdiNtleKGViHJbRGRrSx6FgN0YEE5YKabcJwqUlviEdbgknsGsbDYmoMeRdI2A9MsRlPSrD-Kl3JluXTHBi-pvByRH0NeWoZphitgJR-bL0EQg9W3fvp3KCWdgJPJ0CDMmWxL81S8-85N-m5v3wo-AcFYUV6dIiM2wwcbAuzwlB5a3AS7LZFivNacM41A0jkGhGoVQUwY4ShSTuJKcIfH0D.wEKa9m3e2yRVeKtAKGrioGK0mrRKd0zzfIXSrHPc1OyoAW3Ve4eK-CFlYsJFAA_qGegGzbuq7wjSu-nkZWzinXDxgngBzjBU2CMt6sjBRW2g0ejFgHJwQPTVfVIy939xmNQEK2JY2Z47untKeyktIajaytPchEzHI-uTolg00Mg\"\n",
    "\n",
    "    # Search\n",
    "    url = 'https://api.infodesk.com/data/v1/search'\n",
    "    headers = {'Content-Type': \"application/json\", 'Authorization': f\"Bearer {bearer_token}\",\n",
    "               \"accept\":\"*/*\", \"X-API-KEY\":f\"{api_key}\", 'realm':f\"{realm}\"}\n",
    "\n",
    "    data={\"queryString\":\"\",\n",
    "          \"searchOptions\":{\"startDate\":start_date, \"endDate\":end_date,\n",
    "                           \"includeOldRevisions\":False,},\n",
    "                           \"retrievalOptions\":{\"sortOrder\":\"NEWEST_FIRST\",\n",
    "                                               \"limit\":\"1000\",\n",
    "                                               \"start\":\"0\",\n",
    "                                               \"grouping\":\"NONE\",\n",
    "                                               \"fields\":[\"HEADLINE\",\"DESCRIPTION\",\"LEAD\",\"BYLINE\",\"DOCTIME\",\"TIMESTAMP\",\"CONTENTCATEGORY\",\"CREDITLINE\",\"DATELINE\",\"EDNOTE\",\"GENRE\",\"LANG\",\"PROVIDER\",\"SLUGLINE\",\"ORIGINALURL\",\"ORIGINALID\",\"LOCATION\",\"COUNTRY\",\"ORGANIZATION\",\"INDUSTRY\",\"PUBLICATION\",\"PUBDATE\",\"KEYWORDS\",\"TOPICS\",\"DRUG\",\"DRUGCLASS\",\"MEDFORM\",\"MOA\",\"MEDINDICATION\",\"MEDPHASE\",\"GENETARGET\",\"CONFERENCE\"]}}\n",
    "\n",
    "    if profile: data[\"profile\"] = profile\n",
    "    if channel: data[\"searchOptions\"][\"sources\"] = [{\"CHANNEL\":channel},],\n",
    "    if limit: data[\"retrievalOptions\"][\"limit\"] = str(limit),\n",
    "\n",
    "    print(url)\n",
    "    print(data)\n",
    "    print(headers)\n",
    "\n",
    "    return requests.post(url, json=data, headers=headers, verify=False)\n",
    "    #return \"Error\"\n",
    "\n",
    "\n",
    "def parse(response):\n",
    "    \n",
    "    if response.status_code!=200:\n",
    "        print(f\"Request not successful {response.status_code}: '{response.text}'\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        results = json.loads(response.text)[\"resultSet\"][\"results\"]\n",
    "    except KeyError:\n",
    "        results = []\n",
    "    \n",
    "    print(f\"Request successful: 'Found {len(results)} data points'\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_data(start_date, end_date, **kwargs):\n",
    "\n",
    "    results = [True]\n",
    "\n",
    "    while len(results)!=0:\n",
    "        response = post_request(start_date, end_date, **kwargs)\n",
    "\n",
    "        results = parse(response)\n",
    "\n",
    "        if len(results) > 0:\n",
    "            end_date = results[-1][\"date\"]\n",
    "            date_formatted = pd.to_datetime(end_date)\n",
    "\n",
    "            # Subtract one month from end date\n",
    "            prev_month = date_formatted - relativedelta(months=1)\n",
    "            end_date = f\"{prev_month.year:02d}-{prev_month.month:02d}-{prev_month.day:02d}T{prev_month.hour:02d}:{prev_month.minute:02d}:{prev_month.second:02d}.701Z\"\n",
    "\n",
    "            results = {\n",
    "                \"profile\":profile,\n",
    "                \"results\":results\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            date_formatted = pd.to_datetime(start_date)\n",
    "\n",
    "            # Subtract one month from start date to get new end date\n",
    "            prev_month = date_formatted - relativedelta(months=1)\n",
    "            end_date = f\"{date_formatted.year:02d}-{date_formatted.month:02d}-{date_formatted.day:02d}T{date_formatted.hour:02d}:{date_formatted.minute:02d}:{date_formatted.second:02d}.701Z\"\n",
    "\n",
    "            # Set start date to new date\n",
    "            start_date = f\"{prev_month.year:02d}-{prev_month.month:02d}-{prev_month.day:02d}T{prev_month.hour:02d}:{prev_month.minute:02d}:{prev_month.second:02d}.701Z\"\n",
    "\n",
    "            results = {\n",
    "                \"profile\":profile,\n",
    "                \"results\":results}\n",
    "\n",
    "            break\n",
    "\n",
    "        yield results, start_date, end_date\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = ('6qpp') #'clxj', #'nk93', #'tuc3', #'qkfq', #'duvj', #'uctf', #'u0vc', #'ggnd', #'xb2m', #'dofq', #'4hwz', #'5ls8', #'klhg', #'jmnc', #'kvkg', #'hj7v', #'xfdy', #'9cxo', #'jeh2', #'zojw', #'6qpp') #infomedia channels, change manually\n",
    "start_date, end_date = \"2018-01-01T00:01:00.701Z\", \"2023-05-01T00:01:00.701Z\"\n",
    "#data_directory = inset directory here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for profile in profiles:\n",
    "\n",
    "    args = {\n",
    "        \"channel\":None,\n",
    "        \"profile\":profile,\n",
    "        \"limit\":None,\n",
    "    }\n",
    "\n",
    "    for result, start_date, end_date in get_data(start_date, end_date, **args):\n",
    "\n",
    "        format_end = pd.to_datetime(end_date)\n",
    "        format_start = pd.to_datetime(start_date)\n",
    "\n",
    "        file_path = os.path.join(data_directory, f\"{profile}-{format_end.year:02d}-{format_end.month:02d}-{format_end.day:02d}.json\")\n",
    "\n",
    "        with open(file_path, \"w\") as f:\n",
    "            f.write(json.dumps(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### To CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"final_MEMO_data\"\n",
    "directory = os.fsencode(data_directory)\n",
    "\n",
    "include_fields = {\n",
    "    \"description\": \"strings\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Concatenating all files into one dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "\n",
    "header = [\"profile\", \"date\"].extend(include_fields.keys())\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "\n",
    "    filename = os.fsdecode(file)\n",
    "\n",
    "    if filename.endswith(\".json\"):\n",
    "\n",
    "        path= os.path.join(data_directory, filename)\n",
    "\n",
    "        print(f\"Reading file: {path}\")\n",
    "\n",
    "        with open(path, \"r\") as f:\n",
    "            data = json.loads(f.read())\n",
    "\n",
    "        results = data[\"results\"]\n",
    "        profile = data[\"profile\"]\n",
    "\n",
    "        for match in results:\n",
    "\n",
    "            data = []\n",
    "\n",
    "            raw_date = match[\"date\"]\n",
    "\n",
    "            data.append(str(pd.to_datetime(raw_date)))\n",
    "            data.append(profile)\n",
    "\n",
    "            inverted_fields = {}\n",
    "\n",
    "            try: fields = match.get(\"fields\")\n",
    "            except KeyError: continue\n",
    "\n",
    "            if not fields: continue\n",
    "            for field in fields:\n",
    "\n",
    "                field_id = field[\"id\"]\n",
    "                \n",
    "                if field_id in include_fields:\n",
    "\n",
    "                    inverted_fields[field_id] = field[include_fields[field_id]][0] # Method for extraction should be a lambda in the include fields value\n",
    "                  \n",
    "            \n",
    "            data.extend(map(lambda idx: inverted_fields.get(idx, \"\"), include_fields))\n",
    "\n",
    "            out.append(data)\n",
    "\n",
    "file_path = os.path.join(data_directory, f\"{name}\")\n",
    "\n",
    "with open(file_path, \"w\") as f:\n",
    "    for m in out:\n",
    "        try:\n",
    "            f.write(\"\\t\".join(m)+\"\\n\")\n",
    "        \n",
    "        except UnicodeEncodeError:\n",
    "            print(\"Unicode error:\", m)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
